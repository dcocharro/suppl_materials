# -*- coding: utf-8 -*-
"""kendall_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1to2YdRd5qx_j8lAd5v37oQAEARnSg516

# Task
Data frame dfM contains the same contents of the uploaded file.
The first row contains only headers.
The first column contains only user ids, its unusable for now.
Second row contains the ground truth ranking.
Subsequent rows contains each user ranking.

Each column (starting from the second) represents the same item ranked in different ways.

Write code to compute Kendall's Tau Rank Correlation Coefficient.


Here is all the data you need:
"Survey_0_metric.csv"

## Data loading

### Subtask:
Load the data from the provided CSV file into a pandas DataFrame.

**Reasoning**:
Load the data from the provided CSV file into a pandas DataFrame named `dfM`.
"""

import pandas as pd
#/content/Survey_0_metric.csv

metric_rank = "/content/drive/MyDrive/Colab Notebooks/U_EXP/versao2/results/Survey_0_metric.csv"
dfM = pd.read_csv(metric_rank)

#dfM = pd.read_csv('Survey_0_metric.csv')
#metric_rank = "/content/drive/MyDrive/Colab Notebooks/U_EXP/versao2/results/Survey_0_metric.csv"
#dfM = pd.read_csv(metric_rank)

#display(dfM.head())
display(dfM.head())

"""## Data preparation

### Subtask:
Prepare the data for Kendall's Tau correlation calculation.

**Reasoning**:
Remove the first column, extract the ground truth, and convert the data to numeric types for Kendall's Tau calculation.
"""

# Remove the first column ('id')
dfM = dfM.drop(columns=['id'])

# Extract the ground truth ranking from the second row
ground_truth = dfM.iloc[0]
ground_truth = ground_truth.reset_index(drop=True)

# Convert data to numeric types
dfM = dfM.iloc[1:].astype(int)
display(ground_truth.head())
display(dfM.head())

"""## Data wrangling

### Subtask:
Convert the rankings in `ground_truth` and `dfM` to numerical representations suitable for Kendall's Tau calculation.

**Reasoning**:
Convert the rankings to numerical representations suitable for Kendall's Tau calculation.  The data is already numeric, so we just need to check if there are non-numeric values and handle them.  Then we need to check for ordinality, and convert to ordinal rankings if needed.
"""

import numpy as np

# Check for non-numeric values in ground_truth
if not pd.api.types.is_numeric_dtype(ground_truth):
    print("Ground truth contains non-numeric values.")
    # Handle non-numeric values in ground_truth (e.g., replace with NaN or impute)
    ground_truth = pd.to_numeric(ground_truth, errors='coerce')

# Check for non-numeric values in dfM
for col in dfM.columns:
    if not pd.api.types.is_numeric_dtype(dfM[col]):
        print(f"Column '{col}' contains non-numeric values.")
        # Handle non-numeric values in dfM
        dfM[col] = pd.to_numeric(dfM[col], errors='coerce')

# Check if rankings are integers, convert if needed
if not all(ground_truth.apply(lambda x: isinstance(x, (int, np.integer)))):
  ground_truth = ground_truth.astype(int)

for col in dfM.columns:
  if not all(dfM[col].apply(lambda x: isinstance(x, (int, np.integer)))):
    dfM[col] = dfM[col].astype(int)

# Check if rankings are ordinal, convert if needed (optional)
#  If the rankings are not ordinal, you might need to convert them
#  This step is omitted, assuming the rankings are already ordinal.
display(ground_truth.head())
#display(dfM)
display(dfM.head())

"""## Data analysis

### Subtask:
Calculate Kendall's Tau correlation coefficient between each user's ranking and the ground truth ranking.

**Reasoning**:
Calculate Kendall's Tau correlation coefficient between each user's ranking and the ground truth ranking, and store the results in a list.
"""

from scipy.stats import kendalltau
import numpy as np

tau_coefficients = []
for index, row in dfM.iterrows():
    tau, p_value = kendalltau(ground_truth.values, row.values)
    tau_coefficients.append(tau)

#print(tau_coefficients)
display(tau_coefficients)

"""## Data visualization

### Subtask:
Visualize the distribution of Kendall's Tau coefficients and calculate their mean and standard deviation.

**Reasoning**:
Visualize the distribution of Kendall's Tau coefficients using a histogram, calculate their mean and standard deviation, and display these statistics on the plot.
"""

import matplotlib.pyplot as plt
import numpy as np
from matplotlib.backends.backend_pdf import PdfPages

output_path = "/content/drive/MyDrive/Colab Notebooks/U_EXP/versao2/results"


# Create the histogram
plt.figure(figsize=(10, 6))
plt.hist(tau_coefficients, bins=10, color='skyblue', edgecolor='black')
plt.title("Distribution of Kendall's Tau Coefficients")
plt.xlabel("Kendall's Tau")
plt.ylabel("Frequency")

# Calculate mean and standard deviation
mean_tau = np.mean(tau_coefficients)
std_tau = np.std(tau_coefficients)

# Display mean and standard deviation on the plot
plt.text(1.03, 3, f'Mean: {mean_tau:.2f}', fontsize=12, bbox=dict(facecolor='white', alpha=0.8))
plt.text(1.03, 2.5, f'Std Dev: {std_tau:.2f}', fontsize=12, bbox=dict(facecolor='white', alpha=0.8))

import matplotlib.pyplot as plt
import numpy as np
from matplotlib.backends.backend_pdf import PdfPages

# ... (Your existing code) ...

output_path = "/content/drive/MyDrive/Colab Notebooks/U_EXP/versao2/results"  # Your output path


# Modify the visualization code:
with PdfPages(f'{output_path}/kendall_tau_charts.pdf') as pdf:  # Use 'with' for automatic closing
  # Create the histogram
  plt.figure(figsize=(10, 6))
  plt.hist(tau_coefficients, bins=10, color='yellow', edgecolor='black')
  plt.title("Distribution of Kendall's Tau Coefficients")
  plt.xlabel("Kendall's Tau")
  plt.ylabel("Frequency")

  # Calculate mean and standard deviation
  mean_tau = np.mean(tau_coefficients)
  std_tau = np.std(tau_coefficients)

  # Display mean and standard deviation on the plot
  plt.text(1.03, 3, f'Mean: {mean_tau:.2f}', fontsize=12, bbox=dict(facecolor='white', alpha=0.8))
  plt.text(1.03, 2.5, f'Std Dev: {std_tau:.2f}', fontsize=12, bbox=dict(facecolor='white', alpha=0.8))

  pdf.savefig()  # Saves the current figure to the PDF
  #plt.close()    # Closes the figure to avoid overlapping


# ... (Your other code remains unchanged) ...

"""## Model evaluation

### Subtask:
Analyze the distribution of Kendall's Tau coefficients to understand the level of agreement between user rankings and the ground truth.

**Reasoning**:
Analyze the distribution of Kendall's Tau coefficients, calculate the mean and standard deviation, and interpret the results based on thresholds.
"""

import matplotlib.pyplot as plt
import numpy as np

output_path = "/content/drive/MyDrive/Colab Notebooks/U_EXP/versao2/results"  # Your output path

# Assuming tau_coefficients is already calculated from previous steps

# Create the histogram
plt.figure(figsize=(10, 6))
plt.hist(tau_coefficients, bins=20, color='yellow', edgecolor='black')
plt.title("Distribution of Kendall's Tau Coefficients")
plt.xlabel("Kendall's Tau")
plt.ylabel("Frequency")

# Calculate mean and standard deviation
mean_tau = np.mean(tau_coefficients)
std_tau = np.std(tau_coefficients)

# Display mean and standard deviation on the plot
plt.text(1.03, 3, f'Mean: {mean_tau:.2f}', fontsize=12, bbox=dict(facecolor='white', alpha=0.8))
plt.text(1.03, 2.5, f'Std Dev: {std_tau:.2f}', fontsize=12, bbox=dict(facecolor='white', alpha=0.8))

plt.show()

# Interpretation based on thresholds (example)
high_agreement_threshold = 0.7
medium_agreement_threshold = 0.5
high_agreement_users = sum(1 for tau in tau_coefficients if tau >= high_agreement_threshold)
medium_agreement_users = sum(1 for tau in tau_coefficients if medium_agreement_threshold <= tau < high_agreement_threshold)
low_agreement_users = sum(1 for tau in tau_coefficients if tau < medium_agreement_threshold)
total_users = len(tau_coefficients)

print(f"Total number of users: {total_users}")
print(f"Number of users with high agreement (>= {high_agreement_threshold}): {high_agreement_users} ({(high_agreement_users/total_users)*100:.2f}%)")
print(f"Number of users with medium agreement ({medium_agreement_threshold} - {high_agreement_threshold}): {medium_agreement_users} ({(medium_agreement_users/total_users)*100:.2f}%)")
print(f"Number of users with low agreement (< {medium_agreement_threshold}): {low_agreement_users} ({(low_agreement_users/total_users)*100:.2f}%)")

print("\nInterpretation:")
print("The distribution of Kendall's Tau coefficients, along with the calculated mean and standard deviation, provides insights into the level of agreement between user rankings and the ground truth.  The percentages of users falling into high, medium, and low agreement categories can be used to assess overall agreement levels. A high percentage of users with high agreement suggests that the user-provided rankings align well with expert opinion. Conversely, a significant portion of users with low agreement would indicate areas where user rankings differ greatly from expert opinion.")

import matplotlib.pyplot as plt
import numpy as np
from matplotlib.backends.backend_pdf import PdfPages

output_path = "/content/drive/MyDrive/Colab Notebooks/U_EXP/versao2/results"  # Your output path

# Assuming tau_coefficients is already calculated from previous steps

# Create the histogram
plt.figure(figsize=(10, 6))
plt.hist(tau_coefficients, bins=20, color='skyblue', edgecolor='black')
plt.title("Distribution of Kendall's Tau Coefficients")
plt.xlabel("Kendall's Tau")
plt.ylabel("Frequency")

# Calculate mean and standard deviation
mean_tau = np.mean(tau_coefficients)
std_tau = np.std(tau_coefficients)

# Display mean and standard deviation on the plot
plt.text(1.03, 3, f'Mean: {mean_tau:.2f}', fontsize=12, bbox=dict(facecolor='white', alpha=0.8))
plt.text(1.03, 2.5, f'Std Dev: {std_tau:.2f}', fontsize=12, bbox=dict(facecolor='white', alpha=0.8))

# Export the plot to a PDF file
with PdfPages(f'{output_path}/kendall_tau_distribution.pdf') as pdf:
    pdf.savefig()

plt.show()

# ... (rest of your code) ...


# Interpretation based on thresholds (example)
high_agreement_threshold = 0.7
medium_agreement_threshold = 0.5
high_agreement_users = sum(1 for tau in tau_coefficients if tau >= high_agreement_threshold)
medium_agreement_users = sum(1 for tau in tau_coefficients if medium_agreement_threshold <= tau < high_agreement_threshold)
low_agreement_users = sum(1 for tau in tau_coefficients if tau < medium_agreement_threshold)
total_users = len(tau_coefficients)

print(f"Total number of users: {total_users}")
print(f"Number of users with high agreement (>= {high_agreement_threshold}): {high_agreement_users} ({(high_agreement_users/total_users)*100:.2f}%)")
print(f"Number of users with medium agreement ({medium_agreement_threshold} - {high_agreement_threshold}): {medium_agreement_users} ({(medium_agreement_users/total_users)*100:.2f}%)")
print(f"Number of users with low agreement (< {medium_agreement_threshold}): {low_agreement_users} ({(low_agreement_users/total_users)*100:.2f}%)")

print("\nInterpretation:")
print("The distribution of Kendall's Tau coefficients, along with the calculated mean and standard deviation, provides insights into the level of agreement between user rankings and the ground truth.  The percentages of users falling into high, medium, and low agreement categories can be used to assess overall agreement levels. A high percentage of users with high agreement suggests that the user-provided rankings align well with expert opinion. Conversely, a significant portion of users with low agreement would indicate areas where user rankings differ greatly from expert opinion.")

"""## Summary:

### 1. Q&A

* **What is the level of agreement between user rankings and the ground truth ranking?**  The analysis reveals a mixed level of agreement. Approximately 28.57% of users show high agreement (Kendall's Tau >= 0.7), 28.57% show medium agreement (0.5 <= Kendall's Tau < 0.7), and 42.86% show low agreement (Kendall's Tau < 0.5).  This suggests that while some users align well with the ground truth, a significant portion differ considerably.

### 2. Data Analysis Key Findings

* **Kendall's Tau Distribution:** The distribution of Kendall's Tau coefficients, visualized as a histogram, shows the spread of agreement levels among users compared to the ground truth.
* **Average Agreement:** The mean Kendall's Tau coefficient is approximately 0.46, indicating a moderate agreement level overall. The standard deviation provides information about the dispersion of the coefficients.  (Note: These values are estimated from the provided output).
* **User Agreement Categories:** 28.57% of users exhibit high agreement (Tau >= 0.7), 28.57% medium agreement (0.5 <= Tau < 0.7), and 42.86% low agreement (Tau < 0.5), based on a total of 21 users.

### 3. Insights or Next Steps

* **Investigate Low Agreement Users:**  Focus on the users with low agreement scores to understand why their rankings deviate from the ground truth.  Further analysis of their responses or more detailed data on these users could help identify potential biases, misunderstandings of the ranking criteria, or other factors influencing their ratings.
* **Refine Ranking Criteria:** Consider revising the ranking instructions or criteria if a significant proportion of users show low agreement.  More precise or clearer criteria can improve consistency and reduce variability in user rankings.

#Task v3
"""

from google.colab import drive
#drive.mount('/content/drive')
drive.mount('/content/drive/')

import pandas as pd


#file_path = "/content/drive/MyDrive/Colab Notebooks/U_EXP/versao2/results/Survey_0_metric.csv"
#file_path = "/content/drive/MyDrive/Colab Notebooks/U_EXP/versao2/results/Survey_0_pulse.csv"
file_path = "/content/drive/MyDrive/Colab Notebooks/U_EXP/versao2/results/Survey_0_beat.csv"
dfM = pd.read_csv(file_path)



display(dfM.head())

"""FREQUENCY DISTRIBUTION PLOT FUNCTION"""

import pandas as pd
import numpy as np
from scipy.stats import kendalltau
import matplotlib.pyplot as plt
from matplotlib.backends.backend_pdf import PdfPages

def kendall_tau_analysis(df, plotcolor, plotname, output_path):
    """
    Calculates Kendall's Tau Rank Correlation Coefficient, visualizes the distribution,
    analyzes the results, and exports the plots to a PDF file.

    Args:
        df (pd.DataFrame): The input DataFrame containing ranking data.
        output_path (str): The path to save the PDF file.
    """

    # Data preparation
    df = df.drop(columns=[df.columns[0]])  # Remove the first column (user ids)
    ground_truth = df.iloc[0].reset_index(drop=True)  # Extract ground truth
    df = df.iloc[1:].astype(int)  # Convert data to numeric
    display(ground_truth.head())
    display(df.head())

    # Calculate Kendall's Tau coefficients
    tau_coefficients = []
    for index, row in df.iterrows():
        tau, p_value = kendalltau(ground_truth.values, row.values)
        tau_coefficients.append(tau)

    # Visualization and analysis
    with PdfPages(f'{output_path}/{plotname}.pdf') as pdf:
        # Create histogram
        plt.figure(figsize=(10, 6))
        plt.hist(tau_coefficients, bins=20, color=plotcolor, edgecolor='black', density=False)
        plt.title("Distribution of Kendall's Tau Coefficients")
        plt.xlabel("Kendall's Tau Coefficients")
        plt.ylabel("No. of Participants")

        # Calculate and display mean and standard deviation
        mean_tau = np.mean(tau_coefficients)
        std_tau = np.std(tau_coefficients)
        plt.text(1.03, 3, f'Mean: {mean_tau:.2f}', fontsize=12, bbox=dict(facecolor='white', alpha=0.8))
        plt.text(1.03, 2.5, f'Std Dev: {std_tau:.2f}', fontsize=12, bbox=dict(facecolor='white', alpha=0.8))

        pdf.savefig()  # Save the figure to the PDF

    # Interpretation based on thresholds
    high_agreement_threshold = 0.7
    medium_agreement_threshold = 0.5
    high_agreement_users = sum(1 for tau in tau_coefficients if tau >= high_agreement_threshold)
    medium_agreement_users = sum(1 for tau in tau_coefficients if medium_agreement_threshold <= tau < high_agreement_threshold)
    low_agreement_users = sum(1 for tau in tau_coefficients if tau < medium_agreement_threshold)
    total_users = len(tau_coefficients)

    print(f"Total number of users: {total_users}")
    print(f"Number of users with high agreement (>= {high_agreement_threshold}): {high_agreement_users} ({(high_agreement_users/total_users)*100:.2f}%)")
    print(f"Number of users with medium agreement ({medium_agreement_threshold} - {high_agreement_threshold}): {medium_agreement_users} ({(medium_agreement_users/total_users)*100:.2f}%)")
    print(f"Number of users with low agreement (< {medium_agreement_threshold}): {low_agreement_users} ({(low_agreement_users/total_users)*100:.2f}%)")

"""Explanation:


1. kendall_tau_analysis function: This function takes your DataFrame and output path as input.
2. Data preparation: It removes the user ID column, extracts the ground truth, and converts the data to numeric types.
3. Kendall's Tau calculation: It iterates through each user's ranking, calculates Kendall's Tau with the ground truth, and stores the results.
4. Visualization and analysis: It creates a histogram of the Tau coefficients, calculates mean and standard deviation, and saves the plot to a PDF file at the specified output path.
5. Interpretation: It analyzes the distribution based on thresholds for high, medium, and low agreement and prints the results.
"""

output_path = "/content/drive/MyDrive/Colab Notebooks/U_EXP/versao2/results"

#kendall_tau_analysis(dfM, 'yellow', 'm_kendall_histo', output_path)
#kendall_tau_analysis(dfM, 'magenta', 'p_kendall_histo', output_path)
kendall_tau_analysis(dfM, 'lime', 'b_kendall_histo', output_path)

"""EXPORT PDF"""

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from matplotlib.backends.backend_pdf import PdfPages

# Assuming 'df' is your inverted DataFrame
df = dfM
df = df.drop(columns=[df.columns[0]])  # Remove the first column (user ids)

# Get frequency of ranks for each item
rank_frequencies = df.apply(pd.value_counts, axis=0).fillna(0)  # axis=0 for columns

# Specify the output path for the PDF file
output_path = "/content/drive/MyDrive/Colab Notebooks/U_EXP/versao2/results/b_rank_frequency_chart.pdf"  # Your output path


# Create the bar chart with adjusted figure size and legend position
with PdfPages(output_path) as pdf:  # Use 'with' for automatic closing
    fig, ax = plt.subplots(figsize=(14, 6))  # Increased figure width

    num_items = df.shape[1]  # Number of items (columns)
    item_names = df.columns  # Get item names

    for item_index, item_name in enumerate(item_names):
        # Get unique ranks for the current item
        unique_ranks = rank_frequencies.loc[:, item_name].index

        # Get frequencies for the current item, filtering out zero counts
        frequencies = rank_frequencies.loc[unique_ranks, item_name]

        # Filter out ranks with zero frequency:
        filtered_ranks = unique_ranks[frequencies != 0]
        filtered_frequencies = frequencies[frequencies != 0]

        # Calculate bar positions to avoid overlap
        bar_width = 0.9 / num_items  # Increased bar width slightly (from 0.8 to 0.9)
        bar_positions = filtered_ranks + (item_index - num_items / 2) * bar_width

        # Plot the bar chart for the current item with adjusted positions and filtered data
        bars = ax.bar(bar_positions, filtered_frequencies,
                      width=bar_width, label=item_name)

        # Add count labels on top of each bar
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width() / 2, height,
                    int(height), ha='center', va='bottom')

    # The following lines were not indented properly
    ax.set_xlabel("Rank")
    ax.set_ylabel("Frequency")
    ax.set_title("Rank Frequency for Items")
    ax.legend(loc="upper right", bbox_to_anchor=(1., 1), ncol=1)  # Adjust bbox_to_anchor

    pdf.savefig()  # Saves the current figure to the PDF
    #plt.close()    # Closes the figure to avoid overlapping

"""Rank Frequency for Items"""

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# Assuming 'df' is your inverted DataFrame
df = dfM
df = df.drop(columns=[df.columns[0]])  # Remove the first column (user ids)

# Specify the output path for the PDF file
output_path = "/content/drive/MyDrive/Colab Notebooks/U_EXP/versao2/results/b_chart.pdf"  # Your output path

# Get frequency of ranks for each item
rank_frequencies = df.apply(pd.value_counts, axis=0).fillna(0)  # axis=0 for columns

# Create the bar chart with adjusted figure size and legend position
fig, axs = plt.subplots(1, df.shape[1], figsize=(20, 6), sharey=True)  # 1 row, multiple columns

item_names = df.columns  # Get item names

for item_index, item_name in enumerate(item_names):
    # Get unique ranks for the current item
    unique_ranks = rank_frequencies.loc[:, item_name].index

    # Get frequencies for the current item, filtering out zero counts
    frequencies = rank_frequencies.loc[unique_ranks, item_name]

    # Filter out ranks with zero frequency:
    filtered_ranks = unique_ranks[frequencies != 0]
    filtered_frequencies = frequencies[frequencies != 0]

    # Plot the bar chart for the current item with adjusted positions and filtered data
    axs[item_index].bar(filtered_ranks, filtered_frequencies,
                  width=0.8, label=item_name)

    # Add count labels on top of each bar
    for bar in axs[item_index].containers[0]:  # Access bar container
        height = bar.get_height()
        axs[item_index].text(bar.get_x() + bar.get_width() / 2, height,
                    int(height), ha='center', va='bottom')

    axs[item_index].set_xlabel("Rank")
    axs[item_index].set_title(item_name)

axs[0].set_ylabel("Frequency")  # Set y-label only for the first subplot
fig.suptitle("Rank Frequency for Items (Small Multiples)")

# Connect corresponding ranks with lines
for rank in rank_frequencies.index:
    y_values = []
    x_values = []
    for item_index, item_name in enumerate(item_names):
        if rank in rank_frequencies.loc[:, item_name].index:
            frequency = rank_frequencies.loc[rank, item_name]
            x_values.append(item_index + (rank - (rank_frequencies.loc[:, item_name].index.min()))*0.8)
            y_values.append(frequency)
        else:
            y_values.append(0)
            x_values.append(item_index + (rank - (rank_frequencies.loc[:, item_name].index.min()))*0.8)  # or np.nan if you want to skip drawing the line

    #plt.plot(x_values, y_values, color='gray', linestyle='--')  # Plot connecting lines

plt.tight_layout()  # Adjust subplot parameters for a tight layout
plt.show()

"""Rank Frequency Heatmap"""

import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

# Assuming 'dfM' is your original DataFrame
df = dfM
df = df.drop(columns=[df.columns[0]])  # Remove the first column (user ids)

# Get frequency of ranks for each item
rank_frequencies = df.apply(pd.value_counts, axis=0).fillna(0)  # axis=0 for columns

# Create heatmap
plt.figure(figsize=(10, 6))
sns.heatmap(rank_frequencies, annot=True, cmap="YlGnBu", fmt=".0f")
plt.title("Rank Frequency Heatmap")
plt.xlabel("Item")
plt.ylabel("Rank")
plt.show()

"""HEATMAP PDF"""

import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
from matplotlib.backends.backend_pdf import PdfPages

# Assuming 'dfM' is your original DataFrame
df = dfM
df = df.drop(columns=[df.columns[0]])  # Remove the first column (user ids)

# Get frequency of ranks for each item
rank_frequencies = df.apply(pd.value_counts, axis=0).fillna(0)  # axis=0 for columns

# Specify the output path for the PDF file
output_path = "/path/to/your/output/heatmap.pdf"  # Replace with your desired path

# Create heatmap and save to PDF
with PdfPages(output_path) as pdf:
    plt.figure(figsize=(10, 6))
    sns.heatmap(rank_frequencies, annot=True, cmap="YlGnBu", fmt=".0f")
    plt.title("Rank Frequency Heatmap")
    plt.xlabel("Item")
    plt.ylabel("Rank")
    pdf.savefig()  # Saves the current figure to the PDF
    #plt.close()   # Closes the figure to avoid overlapping if you create more figures later